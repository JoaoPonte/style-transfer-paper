{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete code to replicate our results\n",
    "This code isn't really user-friendly, but it is somewhat straight-forward to use, since you should only change some variables to test a lot of possibilities ;)\n",
    "\n",
    "The pytorch part was based on the official pytorch tutorial for neural style transfer (https://pytorch.org/tutorials/advanced/neural_style_tutorial.html)\n",
    "\n",
    "I ran the code using the following \n",
    "\n",
    "First, we need to load all libraries and define some important functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "from time import perf_counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from os import walk\n",
    "from pathlib import Path\n",
    "\n",
    "def format_time(sec):\n",
    "    \"\"\"function just to format seconds to a nice and readable string\"\"\"\n",
    "    h = sec // 3600\n",
    "    sec -= h*3600\n",
    "    m = sec // 60\n",
    "    sec -= m*60\n",
    "    \n",
    "    return f'{int(h):02d}h {int(m):02d}m {sec:.2f}'\n",
    "\n",
    "# runs the code using cuda if a nvidia gpu is avalable\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# reconvert from tensor to a PIL image\n",
    "unloader = transforms.ToPILImage()  # reconvert into PIL image\n",
    "\n",
    "def tensor2pil(tensor):\n",
    "    \"\"\"function to convert tensor to PIL image\"\"\"\n",
    "    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it\n",
    "    image = image.squeeze(0)      # remove the fake batch dimension\n",
    "    image = unloader(image)\n",
    "    return image\n",
    "\n",
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target,):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.loss = F.mse_loss(input, self.target)\n",
    "        return input\n",
    "\n",
    "def gram_matrix(input):\n",
    "    a, b, c, d = input.size()  # a=batch size(=1)\n",
    "\n",
    "    features = input.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n",
    "\n",
    "    G = torch.mm(features, features.t())  # compute the gram product\n",
    "    return G.div(a * b * c * d)\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target_feature):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = gram_matrix(target_feature).detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        G = gram_matrix(input)\n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        return input\n",
    "\n",
    "class TotalVariationLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TotalVariationLoss, self).__init__()\n",
    "\n",
    "    def forward_bkp(self, input):\n",
    "        bsize, chan, height, width = input.size()\n",
    "        dy = torch.abs(input[:,:,1:,:] - input[:,:,:-1,:])\n",
    "        dyhat = torch.abs(input[:,:,1:,:] - input[:,:,:-1,:])\n",
    "        error = torch.norm(dy - dyhat, 1)\n",
    "        self.loss = error / height\n",
    "        return input\n",
    "    \n",
    "    def forward(self, input):\n",
    "        bsize, chan, height, width = input.size()\n",
    "        a = torch.pow(input[:,:,:-1,:-1] - input[:,:,1:,:-1], 2.)\n",
    "        b = torch.pow(input[:,:,:-1,:-1] - input[:,:,:-1,1:], 2.)\n",
    "        self.loss = torch.sum(torch.pow(a+b, 1.25))\n",
    "        return input\n",
    "\n",
    "# this loads the pre trained VGG19 model, which was trained using ImageNet dataset\n",
    "cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "\n",
    "\n",
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
    "\n",
    "# create a module to normalize input image so we can easily put it in a\n",
    "# nn.Sequential\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        # .view the mean and std to make them [C x 1 x 1] so that they can\n",
    "        # directly work with image Tensor of shape [B x C x H x W].\n",
    "        # B is batch size. C is number of channels. H is height and W is width.\n",
    "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        # normalize img\n",
    "        return (img - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS: VGG19 has in total 19 layers and we were supposed to use them all, but since I saw that the last ten layers didn't contribute significantly to the output image, I cut them off and we can only set the weight for the 10 first layers.  \n",
    "We can see below which layers I chose and naturally you can choose all of them, another configuration, only one, ...  \n",
    "\n",
    "Note that I chose the conv_5 layer as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desired depth layers to compute style/content losses :\n",
    "content_layers_default = ['conv_5']\n",
    "style_layers_default = ['conv_1', \n",
    "                        'conv_2', \n",
    "                        'conv_3', \n",
    "                        'conv_4', \n",
    "                        'conv_5',\n",
    "                        'conv_6', \n",
    "                        'conv_7', \n",
    "                        'conv_8', \n",
    "                        'conv_9', \n",
    "                        'conv_10']\n",
    "\n",
    "\"\"\"                     'conv_11', \n",
    "                        'conv_12', \n",
    "                        'conv_13', \n",
    "                        'conv_14', \n",
    "                        'conv_15',\n",
    "                        'conv_16'\"\"\"\n",
    "\n",
    "def get_style_model_and_losses(cnn, \n",
    "                               normalization_mean, \n",
    "                               normalization_std,\n",
    "                               style_img, \n",
    "                               content_img,\n",
    "                               content_layers=content_layers_default,\n",
    "                               style_layers=style_layers_default):\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "\n",
    "    # normalization module\n",
    "    normalization = Normalization(normalization_mean, normalization_std).to(device)\n",
    "\n",
    "    # just in order to have an iterable access to or list of content/syle\n",
    "    # losses\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "\n",
    "    # assuming that cnn is a nn.Sequential, so we make a new nn.Sequential\n",
    "    # to put in modules that are supposed to be activated sequentially\n",
    "    model = nn.Sequential(normalization)\n",
    "\n",
    "    i = 0  # increment every time we see a conv\n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i += 1\n",
    "            name = 'conv_{}'.format(i)\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = 'relu_{}'.format(i)\n",
    "            # The in-place version doesn't play very nicely with the ContentLoss\n",
    "            # and StyleLoss we insert below. So we replace with out-of-place\n",
    "            # ones here.\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = 'pool_{}'.format(i)\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = 'bn_{}'.format(i)\n",
    "        else:\n",
    "            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
    "\n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        if name in content_layers:\n",
    "            # add content loss:\n",
    "            target = model(content_img).detach()\n",
    "            content_loss = ContentLoss(target)\n",
    "            model.add_module(\"content_loss_{}\".format(i), content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "\n",
    "        if name in style_layers:\n",
    "            # add style loss:\n",
    "            target_feature = model(style_img).detach()\n",
    "            style_loss = StyleLoss(target_feature)\n",
    "            model.add_module(\"style_loss_{}\".format(i), style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "    \n",
    "    #target = model(content_img).detach()\n",
    "    total_variation_loss = TotalVariationLoss()\n",
    "    model.add_module('tv_loss', total_variation_loss)\n",
    "    \n",
    "    # now we trim off the layers after the last content and style losses\n",
    "    for i in range(len(model) - 1, -1, -1):\n",
    "        if isinstance(model[i], ContentLoss) or \\\n",
    "        isinstance(model[i], StyleLoss) or \\\n",
    "        isinstance(model[i], TotalVariationLoss):\n",
    "            break\n",
    "    \n",
    "    model = model[:(i + 1)]\n",
    "\n",
    "    return model, style_losses, content_losses, total_variation_loss\n",
    "\n",
    "def get_input_optimizer(input_img):\n",
    "    # this line to show that input is a parameter that requires a gradient\n",
    "    optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
    "    return optimizer\n",
    "\n",
    "def run_style_transfer(cnn, \n",
    "                       normalization_mean, \n",
    "                       normalization_std,\n",
    "                       content_img, \n",
    "                       style_img, \n",
    "                       input_img, \n",
    "                       num_steps=300,\n",
    "                       style_weight=1e5,\n",
    "                       style_sub_weights=[1]*16, \n",
    "                       content_weight=1,\n",
    "                       tv_weight=1,\n",
    "                       verbose=3,\n",
    "                       content_layers=content_layers_default):\n",
    "    \"\"\"Run the style transfer.\"\"\"\n",
    "    \n",
    "    if verbose==3:\n",
    "        print('Building the style transfer model..')\n",
    "    \n",
    "    model, style_losses, content_losses, total_variation_loss = get_style_model_and_losses(cnn,\n",
    "        normalization_mean, normalization_std, style_img, content_img, content_layers=content_layers)\n",
    "    optimizer = get_input_optimizer(input_img)\n",
    "\n",
    "    if verbose==3:\n",
    "        print('Optimizing..')\n",
    "    run = [0]\n",
    "    while run[0] <= num_steps:\n",
    "\n",
    "        def closure():\n",
    "            # correct the values of updated input image\n",
    "            input_img.data.clamp_(0, 1)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            model(input_img)\n",
    "            style_score = 0\n",
    "            content_score = 0\n",
    "    \n",
    "            for n,sl in enumerate(style_losses):\n",
    "                style_score += style_sub_weights[n]*sl.loss\n",
    "            for cl in content_losses:\n",
    "                content_score += cl.loss\n",
    "            total_variation_score = tv_weight*total_variation_loss.loss\n",
    "    \n",
    "            style_score *= style_weight\n",
    "            content_score *= content_weight\n",
    "            \n",
    "            loss = 0\n",
    "            loss += style_score if style_weight != 0 else 0\n",
    "            loss += content_score if content_weight != 0 else 0\n",
    "            loss += total_variation_score if tv_weight != 0 else 0\n",
    "            loss.backward()\n",
    "            \n",
    "            run[0] += 1\n",
    "            if run[0] % 100 == 0:\n",
    "                if verbose>=1:\n",
    "                    print(\"run {} - total loss {:.4f}\".format(run[0], loss.item()))\n",
    "                if verbose>=2:\n",
    "                    print('Style Loss : {:4f} Content Loss: {:4f} Total Variation Loss: {:4f}'.format(\n",
    "                    style_score.item(), content_score.item(), total_variation_score.item()))\n",
    "                if verbose>=1:\n",
    "                    print()\n",
    "    \n",
    "            return style_score + content_score + total_variation_score\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    # a last correction...\n",
    "    input_img.data.clamp_(0, 1)\n",
    "\n",
    "    return input_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until here, we declared only functions and some variables, but nothing really interesting...\n",
    "\n",
    "But now, we can run experiments!\n",
    "Above, we have some vars to declare.\n",
    "\n",
    "- style_weight: This is the weight we will put to the style loss function during the optimization step\n",
    "- content_weight: Almost the same as style_weight, but for the content loss function.\n",
    "- tv_weight: Almost the same agein, but for the total variation loss function, which I didn't use on my paper\n",
    "- num_iterations: number of iterations of the optimizing step\n",
    "- verbose: 0 for no messages, 1-2 to some level of messages and 3 for see all the details while the network works ;)\n",
    "\n",
    "Another group of variables are a little bit complexer than the others, but not so much.  \n",
    "The style_sub_weights are the weights for the 10 first layers from the VGG 19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_weight = 1e4\n",
    "content_weight = 1\n",
    "tv_weight = 0\n",
    "num_iterations = 200\n",
    "verbose = 0\n",
    "\n",
    "style_sub_weights = [1., 1.,\n",
    "                     1., 1.,\n",
    "                     1., 1., 1., 1.,\n",
    "                     1., 1.]\n",
    "\n",
    "# used only to save the style transfer output\n",
    "style_weights_str = '-'.join([f'{i:02.1f}' for i in style_sub_weights])\n",
    "content_name = 'cx9' #'.'.join(end_img_base.split('/')[-1].split('.')[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the part we set out input data.\n",
    "\n",
    "My version of the code support \"3D\" input, in other words, a serie of 2D images forming a 3D block.  \n",
    "Considering that I can not share the data, I tried to make this part of the code more understandable and easy to change.\n",
    "\n",
    "The main loading part is intended to load a dataset formed for a path which contains some images inside.  \n",
    "If you want to change this do load only one image, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the ST in the Z axis\n",
    "img_ref_filename  = '' # including extension\n",
    "output_path = ''\n",
    "\n",
    "# desired size of the output image\n",
    "# use small size if no gpu\n",
    "imsize = (<height>, <width>) if torch.cuda.is_available() else (<reduced height>, <reduced width>)\n",
    "\n",
    "# List of paths to load a dataset\n",
    "# it can be set to load files from as many paths as you wish\n",
    "# the only must-do is name the images in alphabetical order, like img_0001, img_0002, ...\n",
    "images_orig_path = [Path('<path #1>'),\n",
    "                    Path('<path #2>')]\n",
    "\n",
    "# this will load all images inside the paths defined above\n",
    "n = 0\n",
    "orig_sample = np.zeros((553,464,512), dtype=np.uint8)\n",
    "for p in images_orig_path:\n",
    "    files_orig = next(walk(p))[2]\n",
    "    \n",
    "    # remove some annoying file starting with a \".\".\n",
    "    # Well, you can filter as many files as you want here!\n",
    "    files_orig = list(filter(lambda x: x[0] != '.', files_orig))\n",
    "    \n",
    "    files_orig.sort()\n",
    "\n",
    "    for file in files_orig:\n",
    "        img = Image.open(p / file)\n",
    "        img = img.resize((orig_sample.shape[2], orig_sample.shape[1]))\n",
    "        orig_sample[n] = np.array(img)\n",
    "        n += 1\n",
    "        \n",
    "# Filter all values near 0 to reduce some random noise inside pores and outside the sample - if there is any exterior part\n",
    "orig_sample[orig_sample < 15] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = transforms.Compose([\n",
    "    transforms.Resize(imsize),  # scale imported image\n",
    "    transforms.ToTensor()])  # transform it into a torch tensor\n",
    "\n",
    "def image_loader(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    # fake batch dimension required to fit network's input dimensions\n",
    "    image = loader(image).unsqueeze(0)\n",
    "    return image.to(device, torch.float)\n",
    "\n",
    "style_img = image_loader(img_ref_filename)\n",
    "\n",
    "tempo_antes_total = perf_counter()\n",
    "\n",
    "pbar = tqdm(total=orig_sample.shape[0])\n",
    "output_image = np.zeros_like(orig_sample)\n",
    "\n",
    "for n in range(orig_sample.shape[0]):\n",
    "    tempo_antes_parcial = perf_counter()\n",
    "    \n",
    "    content_img = loader(Image.fromarray(orig_sample[n]))\n",
    "    content_img = content_img.unsqueeze(0)\n",
    "    content_img = content_img.to(device, torch.float)\n",
    "    \n",
    "    base_img = content_img.clone()\n",
    "    job_string = f'{content_name}-sl-{n:03d}-stw-{style_weight:.1e}-stws-{style_weights_str}-cw-{content_weight:.1f}-tvw-{tv_weight:.1f}-it-{num_iterations}.bmp'\n",
    "    if verbose >= 1:\n",
    "        pbar.set_description(f'Running {job_string}')\n",
    "\n",
    "    output = run_style_transfer(cnn, \n",
    "                                cnn_normalization_mean, \n",
    "                                cnn_normalization_std,\n",
    "                                content_img.clone(), \n",
    "                                style_img.clone(), \n",
    "                                base_img.clone(), \n",
    "                                num_steps=num_iterations, \n",
    "                                style_weight=style_weight,\n",
    "                                style_sub_weights=style_sub_weights,\n",
    "                                content_weight=content_weight,\n",
    "                                tv_weight=tv_weight,\n",
    "                                verbose=verbose)\n",
    "\n",
    "    img_output = tensor2pil(output)\n",
    "    output_image[n] = np.array(img_output)\n",
    "    img_output.save(f'{output_path}/{job_string}')\n",
    "    pbar.update(1)\n",
    "\n",
    "print(f'Total ellapsed time: {format_time(perf_counter() - tempo_antes_total)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can explore a little of the output from the style transfer method.  \n",
    "As the main subject of my paper is porosity, you can choose a threshold and calculate the porosity after and before ST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze porosity\n",
    "threshold = 100\n",
    "image_number = 0\n",
    "\n",
    "img_original_np = orig_sample[image_number]\n",
    "img_output_np = output_image[image_number]\n",
    "\n",
    "poro_orig = np.where(img_original_np <= threshold)[0].size/img_original_np.size\n",
    "poro_output = np.where(img_output_np <= threshold)[0].size/img_output_np.size\n",
    "\n",
    "print('Porosity:')\n",
    "print(f' - Original: {100*poro_orig:.2f}%')\n",
    "print(f' -   Styled: {100*poro_output:.2f}%')\n",
    "\n",
    "segm_original, segm_st = img_original_np.copy(), img_output_np.copy()\n",
    "\n",
    "segm_original[segm_original <= threshold] = 0\n",
    "segm_original[segm_original > threshold] = 1\n",
    "\n",
    "segm_st[segm_st <= threshold] = 0\n",
    "segm_st[segm_st > threshold] = 1\n",
    "\n",
    "plt.figure(0, figsize=(15,15))\n",
    "plt.subplot(221)\n",
    "plt.title('Original image')\n",
    "plt.imshow(img_original_np, cmap='gray')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.title('Stylized image')\n",
    "plt.imshow(img_output_np, cmap='gray')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.title('Original - Segmented')\n",
    "plt.imshow(segm_original, cmap='gray')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.title('Stylized - Segmented')\n",
    "plt.imshow(segm_st, cmap='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
